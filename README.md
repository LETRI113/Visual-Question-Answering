ğŸ§  Visual Question Answering (VQA) â€“ ResNet50 + LSTM + Attention
ğŸ“˜ Introduction

This project tackles the Visual Question Answering (VQA) problem â€” combining Computer Vision and Natural Language Processing to predict textual answers from an image-question pair.

Goal: build a baseline VQA model using

ResNet-50 for visual features

LSTM for question encoding

Attention mechanism for multimodal fusion

ğŸ“‚ Dataset:
Visual Question Answering â€“ Computer Vision & NLP (Kaggle)

âš™ï¸ Methodology
ğŸ–¼ï¸ Image Encoder â€“ ResNet-50

Pretrained on ImageNet.

Removed final classification layer.

Extracted 2048-D visual features representing key image regions.

ğŸ’¬ Question Encoder â€“ LSTM

Text preprocessing: tokenization, padding, vocabulary building.

Embedding size: 256.

Encoded by LSTM with 512 hidden units to capture sequential dependencies.

ğŸ¯ Attention Mechanism

Computes similarity between question and image features.

Produces attention weights over image regions.

Weighted visual representation is concatenated with the question vector.

ğŸ§© Answer Decoder

Combines multimodal vector (image + text).

Predicts the answer sequence or class using a fully connected layer.

ğŸ§ª Experiment Results

| Metric                        |          Result          |
| :---------------------------- | :----------------------: |
| **Top-1 Validation Accuracy** | **59.04% (3164 / 5359)** |
| **Token-level Accuracy**      |        **0.5962**        |
| **Average F1-score**          |        **0.2834**        |
| **Best Eval Loss**            |        **1.9746**        |
| **Train Loss (final)**        |        **1.1255**        |

ğŸ§  The model shows stable convergence and moderate performance for a baseline.
The attention mechanism improves interpretability and focus on relevant image regions, though gains over the baseline are modest.

âš™ï¸ Training Details

Optimizer: AdamW

Learning rate: 1e-4

Dropout: 0.3

Batch size: 64

Epochs: up to 50 (early stopping after 41)

Loss function: Cross-Entropy

ğŸš€ Future Work

Integrate Transformer-based encoders (ViT + BERT)

Improve F1-score using better text generation decoders

Apply multimodal fusion layers (e.g., MLP + attention heads)

Evaluate using BLEU / ROUGE / CIDEr for long-form answers


   [áº¢NH] â”€â–º CNN (ResNet50) â”€â–º image_feat (512)
                         â”‚
                         â–¼
   [CÃ‚U Há»I] â”€â–º LSTM Encoder â”€â–º question_feat (512)
                         â”‚
                         â–¼
               Concatenate â”€â–º combined_feat (1024)
                         â”‚
                         â–¼
        [Decoder hidden state] â”€â”€â”€â”
                                  â”‚
                                  â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Attention     â”‚
                         â”‚  (Linear + Ïƒ) â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                        context vector (512)
                                â”‚
                                â–¼
                      Answer Decoder (LSTM)
                                â”‚
                                â–¼
                         Sinh ra cÃ¢u tráº£ lá»i
